{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.6 - AzureML",
      "language": "python",
      "name": "python3-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "05c - Transfer Learning (PyTorch).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7b98fb74318d4ede853f5ae932431014": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d648e0ea3fd7420296fa18251a38457c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7256fbed3aec4371b0780f64d52f6e41",
              "IPY_MODEL_f91e24daa51443f9a5646f20bd6c4866"
            ]
          }
        },
        "d648e0ea3fd7420296fa18251a38457c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7256fbed3aec4371b0780f64d52f6e41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_14da3d63583b47a2a9225cb889b615aa",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 87306240,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 87306240,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_280ea6a697814abfa50a7373d074581b"
          }
        },
        "f91e24daa51443f9a5646f20bd6c4866": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0ffbbc90d69e42caabe8c4f800112a80",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 83.3M/83.3M [03:30&lt;00:00, 415kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7bf72609c1244bf2a44fae08d9894c26"
          }
        },
        "14da3d63583b47a2a9225cb889b615aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "280ea6a697814abfa50a7373d074581b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0ffbbc90d69e42caabe8c4f800112a80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7bf72609c1244bf2a44fae08d9894c26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/franmarq/ml-notes/blob/main/05c_Transfer_Learning_(PyTorch).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqZrA1qmJ0BY",
        "outputId": "8284eb37-f14d-474e-e2f5-23f251e368fd"
      },
      "source": [
        "!git clone https://github.com/microsoftdocs/ml-basics"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ml-basics'...\n",
            "remote: Enumerating objects: 2030, done.\u001b[K\n",
            "remote: Counting objects: 100% (320/320), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 2030 (delta 282), reused 252 (delta 252), pack-reused 1710\u001b[K\n",
            "Receiving objects: 100% (2030/2030), 8.26 MiB | 13.28 MiB/s, done.\n",
            "Resolving deltas: 100% (789/789), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "657ECoPJJqXy"
      },
      "source": [
        "# Transfer Learning\n",
        "\n",
        "A Convolutional Neural Network (CNN) for image classification is made up of multiple layers that extract features, such as edges, corners, etc; and then use a final fully-connected layer to classify objects based on these features. You can visualize this like this:\n",
        "\n",
        "<table>\n",
        "    <tr><td rowspan=2 style='border: 1px solid black;'>&#x21d2;</td><td style='border: 1px solid black;'>Convolutional Layer</td><td style='border: 1px solid black;'>Pooling Layer</td><td style='border: 1px solid black;'>Convolutional Layer</td><td style='border: 1px solid black;'>Pooling Layer</td><td style='border: 1px solid black;'>Fully Connected Layer</td><td rowspan=2 style='border: 1px solid black;'>&#x21d2;</td></tr>\n",
        "    <tr><td colspan=4 style='border: 1px solid black; text-align:center;'>Feature Extraction</td><td style='border: 1px solid black; text-align:center;'>Classification</td></tr>\n",
        "</table>\n",
        "\n",
        "*Transfer Learning* is a technique where you can take an existing trained model and re-use its feature extraction layers, replacing its final classification layer with a fully-connected layer trained on your own custom images. With this technique, your model benefits from the feature extraction training that was performed on the base model (which may have been based on a larger training dataset than you have access to) to build a classification model for your own specific set of object classes.\n",
        "\n",
        "How does this help? Well, think of it this way. Suppose you take a professional tennis player and a complete beginner, and try to teach them both how to play raquetball. It's reasonable to assume that the professional tennis player will be easier to train, because many of the underlying skills involved in raquetball are already learned. Similarly, a pre-trained CNN model may be easier to train to classify specific set of objects because it's already learned how to identify the features of common objects, such as edges and corners. Fundamentally, a pre-trained model can be a great way to produce an effective classifier even when you have limited data with which to train it.\n",
        "\n",
        "In this notebook, we'll see how to implement transfer learning for a classification model using PyTorch.\n",
        "\n",
        "## Install and import libraries\n",
        "\n",
        "First, let's install and import the PyTorch libraries we're going to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEpKjGYZJqX7",
        "outputId": "9736d39a-2e20-49ca-bd64-24d420f834c8"
      },
      "source": [
        "!pip install torch==1.7.1+cpu torchvision==0.8.2+cpu torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cpu\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cpu/torch-1.7.1%2Bcpu-cp37-cp37m-linux_x86_64.whl (159.4MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 159.4MB 93kB/s \n",
            "\u001b[?25hCollecting torchvision==0.8.2+cpu\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.8.2%2Bcpu-cp37-cp37m-linux_x86_64.whl (11.8MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11.8MB 40.7MB/s \n",
            "\u001b[?25hCollecting torchaudio==0.7.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/16/ecdb9eb09ec6b8133d6c9536ea9e49cd13c9b5873c8488b8b765a39028da/torchaudio-0.7.2-cp37-cp37m-manylinux1_x86_64.whl (7.6MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.6MB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cpu) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cpu) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2+cpu) (7.1.2)\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1+cpu which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "  Found existing installation: torchvision 0.9.1+cu101\n",
            "    Uninstalling torchvision-0.9.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.9.1+cu101\n",
            "Successfully installed torch-1.7.1+cpu torchaudio-0.7.2 torchvision-0.8.2+cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3ukd9umJqX9",
        "outputId": "be023800-7dfc-4428-9633-5b1591da637c"
      },
      "source": [
        "# Import PyTorch libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Other libraries we'll use\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"Libraries imported - ready to use PyTorch\", torch.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Libraries imported - ready to use PyTorch 1.7.1+cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twhW8YttJqX_"
      },
      "source": [
        "## Prepare the base model\n",
        "\n",
        "To use transfer learning, we need a base model from which we can use the trained feature extraction layers. The ***resnet*** model is an CNN-based image classifier that has been pre-trained using a huge dataset containing a large number of images of 1000 classes of object, so let's download it and take a look at its layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7b98fb74318d4ede853f5ae932431014",
            "d648e0ea3fd7420296fa18251a38457c",
            "7256fbed3aec4371b0780f64d52f6e41",
            "f91e24daa51443f9a5646f20bd6c4866",
            "14da3d63583b47a2a9225cb889b615aa",
            "280ea6a697814abfa50a7373d074581b",
            "0ffbbc90d69e42caabe8c4f800112a80",
            "7bf72609c1244bf2a44fae08d9894c26"
          ]
        },
        "id": "ycG2sIzsJqYC",
        "outputId": "62f80d9d-6408-4a74-9d25-c389ed2bc989"
      },
      "source": [
        "# Load the model (download if not already present)\n",
        "model = torchvision.models.resnet34(pretrained=True)\n",
        "\n",
        "print(model)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b98fb74318d4ede853f5ae932431014",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=87306240.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (4): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (5): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "megH09ejJqYD"
      },
      "source": [
        "## Prepare the image data\n",
        "\n",
        "The pretrained model has many layers, starting with a convolutional layer that starts the feature extraction process from image data, and ending with a fully-connected linear layer that maps the extracted features to 1000 class labels.\n",
        "\n",
        "For feature extraction to work with our own images, we  need to ensure that the image data we use to train our prediction layer has the same number of features (pixel values) as the images originally used to train the feaure extraction layers. The model does not explicitly give this size, but the first convolutional layer applies by a 7x7 kernel with a stride of 2x2 and results in 64 feature values, so the original size must be 64 x (7 &div; 2), which is 224.\n",
        "\n",
        "PyTorch includes functions for loading and transforming data. We'll use these to create an iterative loader for training data, and a second iterative loader for test data (which we'll use to validate the trained model). The loaders will transform the image data to match the format used to train the original resnet CNN model, convert the image data into *tensors* (which are the core data structure used in PyTorch), and normalize them.\n",
        "\n",
        "Run the following cell to define the data loaders and list the classes for our images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FC5YfKd6JqYF",
        "outputId": "ccf3df87-1572-4a45-f9e5-560c9d2d1862"
      },
      "source": [
        "# Function to ingest data using training and test loaders\n",
        "def load_dataset(data_path):\n",
        "    \n",
        "    # Resize to 256 x 256, then center-crop to 224x224 (to match the resnet image size)\n",
        "    transformation = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    # Load all of the images, transforming them\n",
        "    full_dataset = torchvision.datasets.ImageFolder(\n",
        "        root=data_path,\n",
        "        transform=transformation\n",
        "    )\n",
        "    \n",
        "    # Split into training (70%) and testing (30%) datasets)\n",
        "    train_size = int(0.7 * len(full_dataset))\n",
        "    test_size = len(full_dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
        "    \n",
        "    # define a loader for the training data we can iterate through in 30-image batches\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=30,\n",
        "        num_workers=0,\n",
        "        shuffle=False\n",
        "    )\n",
        "    \n",
        "    # define a loader for the testing data we can iterate through in 30-image batches\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=30,\n",
        "        num_workers=0,\n",
        "        shuffle=False\n",
        "    )\n",
        "        \n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "# Now load the images from the shapes folder\n",
        "import os  \n",
        "data_path = 'ml-basics/data/shapes/'\n",
        "\n",
        "# Get the iterative dataloaders for test and training data\n",
        "train_loader, test_loader = load_dataset(data_path)\n",
        "\n",
        "# Get the class names\n",
        "classes = os.listdir(data_path)\n",
        "classes.sort()\n",
        "print('class names:', classes)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "class names: ['circle', 'square', 'triangle']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpWFP2QEJqYK"
      },
      "source": [
        "## Create a prediction layer\n",
        "\n",
        "We downloaded the complete *resnet* model including its final **fc** linear layer. This fully-connected linear layer takes 512 inputs (the extracted features) and produces 1000 outputs (class predictions based on the original training image classes). We need to replace this layer with one that takes the same number of inputs (so we can use the same number of extracted features), but produces a prediction for each of our image classes.\n",
        "\n",
        "We also need to freeze the feature extraction layers to retain the trained weights. Then when we train the model using our images, only the final prediction layer will learn new weight and bias values - the pre-trained weights already learned for feature extraction will remain the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4VK9adFJqYM",
        "outputId": "fbfd4c17-543d-4af1-a0b1-1c38ed84b9e4"
      },
      "source": [
        "# Set the existing feature extraction layers to read-only\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the prediction layer\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(classes))\n",
        "\n",
        "# Now print the full model, which will include the feature extraction layers of the base model and our prediction layer\n",
        "print(model)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (3): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (4): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (5): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SLlBKEnJqYO"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "With the layers of the CNN defined, we're ready to train it using our image data. The weights used in the feature extraction layers from the base resnet model will not be changed by training, only the final linear layer that maps the features to our shape classes will be trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0FfYOpVJqYP",
        "outputId": "6da8ff44-2690-41bb-fca0-9ef5e0296973"
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    print(\"Epoch:\", epoch)\n",
        "    # Process the images in batches\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # Use the CPU or GPU as appropriate\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        # Reset the optimizer\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Push the data forward through the model layers\n",
        "        output = model(data)\n",
        "        \n",
        "        # Get the loss\n",
        "        loss = loss_criteria(output, target)\n",
        "        \n",
        "        # Keep a running total\n",
        "        train_loss += loss.item()\n",
        "        \n",
        "        # Backpropagate\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Print metrics for every 10 batches so we see some progress\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Training set [{}/{} ({:.0f}%)] Loss: {:.6f}'.format(\n",
        "                batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "            \n",
        "    # return average loss for the epoch\n",
        "    avg_loss = train_loss / (batch_idx+1)\n",
        "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
        "    return avg_loss\n",
        "            \n",
        "            \n",
        "def test(model, device, test_loader):\n",
        "    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        batch_count = 0\n",
        "        for data, target in test_loader:\n",
        "            batch_count += 1\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            \n",
        "            # Get the predicted classes for this batch\n",
        "            output = model(data)\n",
        "            \n",
        "            # Calculate the loss for this batch\n",
        "            test_loss += loss_criteria(output, target).item()\n",
        "            \n",
        "            # Calculate the accuracy for this batch\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            correct += torch.sum(target==predicted).item()\n",
        "\n",
        "    # Calculate the average loss and total accuracy for this epoch\n",
        "    avg_loss = test_loss/batch_count\n",
        "    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        avg_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "    # return average loss for the epoch\n",
        "    return avg_loss\n",
        "    \n",
        "    \n",
        "# Now use the train and test functions to train and test the model    \n",
        "\n",
        "device = \"cpu\"\n",
        "if (torch.cuda.is_available()):\n",
        "    # if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n",
        "    device = \"cuda\"\n",
        "print('Training on', device)\n",
        "\n",
        "# Create an instance of the model class and allocate it to the device\n",
        "model = model.to(device)\n",
        "\n",
        "# Use an \"Adam\" optimizer to adjust weights\n",
        "# (see https://pytorch.org/docs/stable/optim.html#algorithms for details of supported algorithms)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Specify the loss criteria\n",
        "loss_criteria = nn.CrossEntropyLoss()\n",
        "\n",
        "# Track metrics in these arrays\n",
        "epoch_nums = []\n",
        "training_loss = []\n",
        "validation_loss = []\n",
        "\n",
        "# Train over 3 epochs (in a real scenario, you'd likely use many more)\n",
        "epochs = 3\n",
        "for epoch in range(1, epochs + 1):\n",
        "        train_loss = train(model, device, train_loader, optimizer, epoch)\n",
        "        test_loss = test(model, device, test_loader)\n",
        "        epoch_nums.append(epoch)\n",
        "        training_loss.append(train_loss)\n",
        "        validation_loss.append(test_loss)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on cpu\n",
            "Epoch: 1\n",
            "Training set [0/840 (0%)] Loss: 1.111545\n",
            "Training set [300/840 (36%)] Loss: 0.651747\n",
            "Training set [600/840 (71%)] Loss: 0.397581\n",
            "Training set: Average loss: 0.663565\n",
            "Validation set: Average loss: 0.380752, Accuracy: 329/360 (91%)\n",
            "\n",
            "Epoch: 2\n",
            "Training set [0/840 (0%)] Loss: 0.393709\n",
            "Training set [300/840 (36%)] Loss: 0.161549\n",
            "Training set [600/840 (71%)] Loss: 0.193524\n",
            "Training set: Average loss: 0.226109\n",
            "Validation set: Average loss: 0.144182, Accuracy: 358/360 (99%)\n",
            "\n",
            "Epoch: 3\n",
            "Training set [0/840 (0%)] Loss: 0.203906\n",
            "Training set [300/840 (36%)] Loss: 0.082709\n",
            "Training set [600/840 (71%)] Loss: 0.181267\n",
            "Training set: Average loss: 0.140600\n",
            "Validation set: Average loss: 0.101836, Accuracy: 355/360 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXiVoBiqJqYS"
      },
      "source": [
        "## View the loss history\n",
        "\n",
        "We tracked average training and validation loss for each epoch. We can plot these to verify that the loss reduced over the training process and to detect *over-fitting* (which is indicated by a continued drop in training loss after validation loss has levelled out or started to increase)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "JJwwVxORJqYU",
        "outputId": "d5a2a7d0-54d9-4e8d-c91d-aca11dbffbe5"
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.plot(epoch_nums, training_loss)\n",
        "plt.plot(epoch_nums, validation_loss)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['training', 'validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhqSIRaLJqYW"
      },
      "source": [
        "## Evaluate model performance\n",
        "\n",
        "We can see the final accuracy based on the test data, but typically we'll want to explore performance metrics in a little more depth. Let's plot a confusion matrix to see how well the model is predicting each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "3V-4P-TyJqYX",
        "outputId": "655955e3-a974-4d00-a363-70699c4314fc"
      },
      "source": [
        "#Pytorch doesn't have a built-in confusion matrix metric, so we'll use SciKit-Learn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Set the model to evaluate mode\n",
        "model.eval()\n",
        "\n",
        "# Get predictions for the test data and convert to numpy arrays for use with SciKit-Learn\n",
        "print(\"Getting predictions from test set...\")\n",
        "truelabels = []\n",
        "predictions = []\n",
        "for data, target in test_loader:\n",
        "    for label in target.cpu().data.numpy():\n",
        "        truelabels.append(label)\n",
        "    for prediction in model.cpu()(data).data.numpy().argmax(1):\n",
        "        predictions.append(prediction) \n",
        "\n",
        "# Plot the confusion matrix\n",
        "cm = confusion_matrix(truelabels, predictions)\n",
        "plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(classes))\n",
        "plt.xticks(tick_marks, classes, rotation=45)\n",
        "plt.yticks(tick_marks, classes)\n",
        "plt.xlabel(\"Predicted Shape\")\n",
        "plt.ylabel(\"Actual Shape\")\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Getting predictions from test set...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sbvu51kgJqYY"
      },
      "source": [
        "## Use the trained model\n",
        "\n",
        "Now that we've trained the model, we can use it to predict the class of an image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "bfvhnMlPJqYa",
        "outputId": "0e8c7882-fdb5-474a-fdcc-6c6de0216ff4"
      },
      "source": [
        "# Function to create a random image (of a square, circle, or triangle)\n",
        "def create_image (size, shape):\n",
        "    from random import randint\n",
        "    import numpy as np\n",
        "    from PIL import Image, ImageDraw\n",
        "    \n",
        "    xy1 = randint(10,40)\n",
        "    xy2 = randint(60,100)\n",
        "    col = (randint(0,200), randint(0,200), randint(0,200))\n",
        "\n",
        "    img = Image.new(\"RGB\", size, (255, 255, 255))\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    \n",
        "    if shape == 'circle':\n",
        "        draw.ellipse([(xy1,xy1), (xy2,xy2)], fill=col)\n",
        "    elif shape == 'triangle':\n",
        "        draw.polygon([(xy1,xy1), (xy2,xy2), (xy2,xy1)], fill=col)\n",
        "    else: # square\n",
        "        draw.rectangle([(xy1,xy1), (xy2,xy2)], fill=col)\n",
        "    del draw\n",
        "    \n",
        "    return img\n",
        "    \n",
        "# Function to predict the class of an image\n",
        "def predict_image(classifier, image):\n",
        "    import numpy\n",
        "    \n",
        "    # Set the classifer model to evaluation mode\n",
        "    classifier.eval()\n",
        "    \n",
        "    # Apply the same transformations as we did for the training images\n",
        "    transformation = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    # Preprocess the image\n",
        "    image_tensor = transformation(image).float()\n",
        "\n",
        "    # Add an extra batch dimension since pytorch treats all inputs as batches\n",
        "    image_tensor = image_tensor.unsqueeze_(0)\n",
        "\n",
        "    # Turn the input into a Variable\n",
        "    input_features = Variable(image_tensor)\n",
        "\n",
        "    # Predict the class of the image\n",
        "    output = classifier(input_features)\n",
        "    index = output.data.numpy().argmax()\n",
        "    return index\n",
        "\n",
        "\n",
        "# Now let's try it with a new image\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import os, shutil\n",
        "\n",
        "# Create a random test image\n",
        "shape = classes[randint(0, len(classes)-1)]\n",
        "img = create_image ((128,128), shape)\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(img)\n",
        "\n",
        "\n",
        "index = predict_image(model, img)\n",
        "print(classes[index])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "triangle\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmsmpqLiJqYb"
      },
      "source": [
        "## Learn more\n",
        "\n",
        "* [PyTorch Documentation](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)"
      ]
    }
  ]
}